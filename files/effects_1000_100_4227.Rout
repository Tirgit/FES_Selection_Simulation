
R version 3.5.0 (2018-04-23) -- "Joy in Playing"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(tidyverse)
── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──
✔ tibble  1.4.2     ✔ purrr   0.2.5
✔ tidyr   0.8.2     ✔ dplyr   0.7.8
✔ readr   1.1.1     ✔ stringr 1.3.1
✔ tibble  1.4.2     ✔ forcats 0.3.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ purrr::lift()   masks caret::lift()
> library(earth)
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
> library(keras)
> library(glmnet)
Loading required package: Matrix

Attaching package: ‘Matrix’

The following object is masked from ‘package:tidyr’:

    expand

Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loaded glmnet 2.0-16

> library(xgboost)

Attaching package: ‘xgboost’

The following object is masked from ‘package:dplyr’:

    slice

> library(ranger)
> library(kknn)

Attaching package: ‘kknn’

The following object is masked from ‘package:caret’:

    contr.dummy

> library(kernlab)

Attaching package: ‘kernlab’

The following object is masked from ‘package:purrr’:

    cross

The following object is masked from ‘package:ggplot2’:

    alpha

> library(Cubist)
> library(ipred)
> library(sessioninfo)
> 
> # ----------------------------------------------------------------
> 
> n <- 1000
> extra <- 100
> cor_value <- 0
> seed <- 4227
> 
> # ----------------------------------------------------------------
> 
> set.seed(seed)
> trn <- SLC14_1(   n, corrVars = extra, corrValue = cor_value)
> lrg <- SLC14_1(10^5, corrVars = extra, corrValue = cor_value)
> 
> # ----------------------------------------------------------------
> 
> ctrl  <- trainControl(method = "cv")
> rctrl <- trainControl(method = "cv", search = "random")
> 
> # ----------------------------------------------------------------
> 
> 
> mod2df <- function(obj, name) {
+   res <- postResample(
+     pred = predict(obj, newdata = lrg), 
+     obs = lrg$y
+   )
+   test <- tibble(
+     RMSE = res["RMSE"],
+     Rsquared = res["Rsquared"],
+     MAE = res["MAE"],
+     time = obj$times$everything[3]/60,
+     model = name
+   )
+   trn <- getTrainPerf(obj)
+   trn <- tibble(
+     RMSE_cv  = trn[,"TrainRMSE"],
+     Rsquared_cv  = trn[,"TrainRsquared"],
+     MAE_cv  = trn[,"TrainMAE"]
+     )
+   bind_cols(test, trn)
+ }
> 
> # not in caret at time of writing:
> predictors.cubist <- function(x, ...)
+   subset(x$usage, Conditions > 0 | Model > 0)$Variable
> 
> 
> # Avoid counting variables in surrogate splits as real variables used
> bagged_predictors <- function(x, surrogate = FALSE, ...) {
+   code <- getModelInfo("rpart", regex = FALSE)[[1]]$predictors
+   eachTree <- lapply(x$mtree,
+                      function(u, surr) code(u$btree, surrogate = surr),
+                      surr = FALSE)
+   unique(unlist(eachTree))
+ }
> 
> vars2df <- function(obj, name) {
+   pred_vars <- 
+     if (obj$method == "cubist") {
+       predictors.cubist(obj$finalModel) 
+     } else {
+         if (obj$method == "treebag") {
+           bagged_predictors(obj$finalModel) 
+         } else {
+           predictors(obj)
+         }
+       } 
+         
+   tibble(
+     Selected = pred_vars,
+     model = rep(name, length(pred_vars))
+   )
+ }
> 
> 
> # ----------------------------------------------------------------
> 
> set.seed(890)
> lm_fit <- train(y ~ . + Var05:Var06 + Var19:Var20, 
+                 data = trn, method = "lm",
+                 trControl = ctrl)
> lm_res <- mod2df(lm_fit, "linear regression")
> 
> # ----------------------------------------------------------------
> 
> set.seed(890)
> mars_fit <- train(y ~ ., data = trn, method = "gcvEarth",
+                   tuneGrid = data.frame(degree = 1:2),
+                   trControl = ctrl)
> mars_res <- mod2df(mars_fit, "MARS")
> 
> # ----------------------------------------------------------------
> 
> set.seed(890)
> svmr_fit <- train(y ~ ., data = trn, method = "svmRadial",
+                   tuneLength = 50,
+                   preProc = c("center", "scale"),
+                   trControl = rctrl)
> svmr_res <- mod2df(svmr_fit, "SVM (radial)")
> 
> # ----------------------------------------------------------------
> 
> set.seed(890)
> mlp_fit <- train(y ~ ., data = trn, method = "mlpKerasDecay",
+                  tuneLength = 50,
+                  preProc = c("center", "scale"),
+                  trControl = rctrl,
+                  epochs = 100, 
+                  verbose = 0)
/Users/max/.virtualenvs/r-tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-12-15 12:57:24.688228: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
/Users/max/.virtualenvs/r-tensorflow/lib/python2.7/site-packages/keras/callbacks.py:96: UserWarning: Method on_batch_begin() is slow compared to the batch update (0.112566). Check your callbacks.
  % delta_t_median)
/Users/max/.virtualenvs/r-tensorflow/lib/python2.7/site-packages/keras/callbacks.py:96: UserWarning: Method on_batch_begin() is slow compared to the batch update (0.131416). Check your callbacks.
  % delta_t_median)
Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
> mlp_res <- mod2df(mlp_fit, "neural network")
> keras::backend()$clear_session()
> 
> # ----------------------------------------------------------------
> 
> knn_grid <- expand.grid(
+   kmax = 1:20,
+   distance = 2,
+   kernel = c("rectangular", "triangular", "gaussian")
+ )
> 
> set.seed(890)
> knn_fit <- train(y ~ ., data = trn, method = "kknn",
+                  tuneGrid = knn_grid,
+                  preProc = c("center", "scale"),
+                  trControl = ctrl)
> knn_res <- mod2df(knn_fit, "KNN")
> 
> # ----------------------------------------------------------------
> 
> cb_grid <- expand.grid(committees = c(1:9, (1:3)*10), 
+                        neighbors = c(0, 1, 3, 5, 7, 9))
> 
> set.seed(890)
> cb_fit <- train(y ~ ., data = trn, method = "cubist",
+                 tuneGrid = cb_grid,
+                 trControl = ctrl)
> cb_res <- mod2df(cb_fit, "Cubist")
> 
> # ----------------------------------------------------------------
> 
> glmn_grid <- expand.grid(alpha = seq(0, 1, by = 0.05),
+                          lambda = 2^(-2:2))
> set.seed(890)
> glmn_fit <- train(y ~ .+ Var05:Var06 + Var19:Var20, 
+                   data = trn, method = "glmnet",
+                   preProc = c("center", "scale"),
+                   tuneGrid = glmn_grid,
+                   trControl = ctrl)
> glmn_res <- mod2df(glmn_fit, "glmnet")
> 
> # ----------------------------------------------------------------
> 
> xgb_grid <- 
+   getModelInfo("xgbTree")[[1]]$grid(len = 50, search = "random") %>%
+   mutate(colsample_bytree = 1)
> 
> set.seed(890)
> xgb_fit <- train(y ~ ., data = trn, method = "xgbTree",
+                  tuneGrid = xgb_grid,
+                  trControl = rctrl)
